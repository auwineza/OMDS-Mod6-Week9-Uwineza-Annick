{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a01f439a",
   "metadata": {},
   "source": [
    "# Week 9 Coding Quiz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2139fa80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting statsmodels\n",
      "  Downloading statsmodels-0.14.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.22.3 in /home/codespace/.local/lib/python3.12/site-packages (from statsmodels) (2.3.1)\n",
      "Requirement already satisfied: scipy!=1.9.2,>=1.8 in /home/codespace/.local/lib/python3.12/site-packages (from statsmodels) (1.16.0)\n",
      "Requirement already satisfied: pandas!=2.1.0,>=1.4 in /home/codespace/.local/lib/python3.12/site-packages (from statsmodels) (2.3.1)\n",
      "Collecting patsy>=0.5.6 (from statsmodels)\n",
      "  Downloading patsy-1.0.2-py2.py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: packaging>=21.3 in /home/codespace/.local/lib/python3.12/site-packages (from statsmodels) (25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/codespace/.local/lib/python3.12/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.12/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/codespace/.local/lib/python3.12/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas!=2.1.0,>=1.4->statsmodels) (1.17.0)\n",
      "Downloading statsmodels-0.14.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (10.4 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading patsy-1.0.2-py2.py3-none-any.whl (233 kB)\n",
      "Installing collected packages: patsy, statsmodels\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2/2\u001b[0m [statsmodels]\u001b[0m [statsmodels]\n",
      "\u001b[1A\u001b[2KSuccessfully installed patsy-1.0.2 statsmodels-0.14.5\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install statsmodels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebb5129d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported. Seed set to 0.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import skew\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "print('Libraries imported. Seed set to 0.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb701b41",
   "metadata": {},
   "source": [
    "## Simulation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "465e07e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000, 1000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def simulate(A=1, B=1, C=10, D=1000):\n",
    "    \"\"\"\n",
    "    Simulate data for Y, X, W.\n",
    "    Parameters:\n",
    "        A: True coefficient on X\n",
    "        B: Noise level in X (variance of U in X=W+U where U~N(0,B))\n",
    "        C: Noise level in Y (std dev of the noise term)\n",
    "        D: Sample size\n",
    "    Returns:\n",
    "        Y, X, W arrays\n",
    "    \"\"\"\n",
    "    W = np.random.normal(0, 1, D)\n",
    "    X = W + np.random.normal(0, B, D)\n",
    "    Y = A * X - W + np.random.normal(0, C, D)\n",
    "    return Y, X, W\n",
    "\n",
    "Y, X, W = simulate()\n",
    "len(Y), len(X), len(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0a3a7f",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "Which of the following is closest to the probability of detecting a nonzero effect of ÔªøXÔªø on ÔªøYÔªø (the t-value of ÔªøXÔªø is greater in absolute value than about 1.96) given A = 1, B = 1, C = 10, D = 1000? Include W in the regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd302326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1 ‚Äî Probability of detection: 0.878\n"
     ]
    }
   ],
   "source": [
    "def detection_probability(A=1, B=1, C=10, D=1000, reps=1000, alpha=0.05):\n",
    "    crit = 1.96  # approx two-sided 5% threshold\n",
    "    t_vals = []\n",
    "    for _ in range(reps):\n",
    "        Y, X, W = simulate(A=A, B=B, C=C, D=D)\n",
    "        XW = sm.add_constant(np.column_stack([X, W]))\n",
    "        model = sm.OLS(Y, XW).fit()\n",
    "        t_vals.append(model.tvalues[1])  # t for X\n",
    "    return np.mean(np.abs(t_vals) > crit)\n",
    "\n",
    "prob_detect_q1 = detection_probability(A=1, B=1, C=10, D=1000, reps=1000)\n",
    "print(f\"Q1 ‚Äî Probability of detection: {prob_detect_q1:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e435cc5",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "Which of the following is closest to the skew of the estimate in that case? (You can compute this using scipy.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21406ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skew of the estimate: 0.014365290670855346\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import skew\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def simulate(A=1, B=1, C=10, D=1000):\n",
    "    W = np.random.normal(0, 1, D)\n",
    "    X = W + np.random.normal(0, B, D)\n",
    "    Y = A * X - W + np.random.normal(0, C, D)\n",
    "    return Y, X, W\n",
    "\n",
    "coefs = []\n",
    "for _ in range(1000):\n",
    "    Y, X, W = simulate(A=1, B=1, C=10, D=1000)\n",
    "    XW = sm.add_constant(np.column_stack([X, W]))\n",
    "    model = sm.OLS(Y, XW).fit()\n",
    "    coefs.append(model.params[1])  # coefficient on X\n",
    "\n",
    "coef_skew = skew(coefs)\n",
    "print(\"Skew of the estimate:\", coef_skew)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1eb73d",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "With A = 1, C = 10, D = 1,000, what value of B is needed to detect that the Data Generating Process (DGP) has a nonzero coefficient for X about 50% of the time? (Choose the closest value.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca31dd57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.2: np.float64(0.102),\n",
       " 0.6: np.float64(0.464),\n",
       " 1.8: np.float64(1.0),\n",
       " 5.4: np.float64(1.0)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def power_for_B(B, reps=500):\n",
    "    return detection_probability(A=1, B=B, C=10, D=1000, reps=reps)\n",
    "\n",
    "Bs = [0.2, 0.6, 1.8, 5.4]\n",
    "powers_B = {B: power_for_B(B) for B in Bs}\n",
    "powers_B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df057bc5",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "With B = 1, C = 10, D = 100 (note the different value of D), what value of A is needed to detect that the DGP has a nonzero coefficient for X about 50% of the time? (Choose the closest value.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7700c957",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.5: np.float64(0.084),\n",
       " 1.0: np.float64(0.156),\n",
       " 2.0: np.float64(0.518),\n",
       " 4.0: np.float64(0.986)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def power_for_A(A, reps=500):\n",
    "    return detection_probability(A=A, B=1, C=10, D=100, reps=reps)\n",
    "\n",
    "As = [0.5, 1.0, 2.0, 4.0]\n",
    "powers_A = {A: power_for_A(A) for A in As}\n",
    "powers_A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506dd413",
   "metadata": {},
   "source": [
    "## Week 9 Homework Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7f6338",
   "metadata": {},
   "source": [
    "1. Write some code that will use a simulation to estimate the standard deviation of the coefficient when there is heteroskedasticity.  \n",
    "Compare these standard errors to those found via statsmodels OLS or a similar linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5c8b886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'emp_sd_beta_hat': np.float64(0.10673306927024596),\n",
       " 'mean_se_conventional': np.float64(0.07510474786708725),\n",
       " 'mean_se_hc1': np.float64(0.111413843051685)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "def sim_hetero_once(n=1000, beta=1.0, sigma=1.0, h=1.5):\n",
    "    \"\"\"\n",
    "    DGP: Y = beta*X + e, with heteroskedastic errors: Var(e|X) = sigma^2*(1 + h*|X|)^2\n",
    "    \"\"\"\n",
    "    X = rng.normal(0, 1, n)\n",
    "    e = rng.normal(0, sigma * (1 + h * np.abs(X)))\n",
    "    Y = beta * X + e\n",
    "    X1 = sm.add_constant(X)\n",
    "    model = sm.OLS(Y, X1).fit()\n",
    "    # HC1 robust\n",
    "    model_hc1 = model.get_robustcov_results(cov_type=\"HC1\")\n",
    "    return {\n",
    "        \"beta_hat\": model.params[1],\n",
    "        \"se_conventional\": model.bse[1],\n",
    "        \"se_hc1\": model_hc1.bse[1]\n",
    "    }\n",
    "\n",
    "def sim_hetero_many(R=1000, n=1000, beta=1.0, sigma=1.0, h=1.5):\n",
    "    out = [sim_hetero_once(n=n, beta=beta, sigma=sigma, h=h) for _ in range(R)]\n",
    "    bhats = np.array([d[\"beta_hat\"] for d in out])\n",
    "    se_conv = np.array([d[\"se_conventional\"] for d in out])\n",
    "    se_hc1  = np.array([d[\"se_hc1\"] for d in out])\n",
    "    return {\n",
    "        \"emp_sd_beta_hat\": bhats.std(ddof=1),\n",
    "        \"mean_se_conventional\": se_conv.mean(),\n",
    "        \"mean_se_hc1\": se_hc1.mean()\n",
    "    }\n",
    "\n",
    "res_het = sim_hetero_many(R=1000, n=1000, beta=1.0, sigma=1.0, h=1.5)\n",
    "res_het\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38e7380",
   "metadata": {},
   "source": [
    "## Heteroskedasticity\n",
    "I ran a simulation where the error variance got bigger as \n",
    "ùëã\n",
    "X increased. When I compared the regular OLS standard errors to the robust ones (HC1), the regular ones were too small. The HC1 robust errors matched the actual spread of the simulated coefficients much better. Basically, when the noise changes with \n",
    "ùëã\n",
    "X, you need robust SEs or you‚Äôll underestimate uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6d2b4b",
   "metadata": {},
   "source": [
    "2. Write some code that will use a simulation to estimate the standard deviation of the coefficient when errors are highly correlated / non-independent.\n",
    "Compare these standard errors to those found via statsmodels OlS or a similar linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bbd70994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'emp_sd_beta_hat': np.float64(0.047369696301241354),\n",
       " 'mean_se_conventional': np.float64(0.04925726002263184),\n",
       " 'mean_se_cluster': np.float64(0.04872828437530312)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "rng = np.random.default_rng(1)\n",
    "\n",
    "def sim_cluster_once(G=100, m=10, beta=1.0, tau=1.2, sigma=1.0):\n",
    "    \"\"\"\n",
    "    DGP: G clusters, m obs per cluster (n = G*m).\n",
    "    X ~ N(0,1); errors have cluster shock u_g and idiosyncratic v.\n",
    "    Y = beta*X + e,  e = u_g + v\n",
    "    \"\"\"\n",
    "    n = G * m\n",
    "    g_id = np.repeat(np.arange(G), m)\n",
    "    X = rng.normal(0, 1, n)\n",
    "    u = rng.normal(0, tau, G)  # cluster shocks\n",
    "    v = rng.normal(0, sigma, n)\n",
    "    e = u[g_id] + v\n",
    "    Y = beta * X + e\n",
    "    X1 = sm.add_constant(X)\n",
    "    model = sm.OLS(Y, X1).fit()\n",
    "    # Cluster-robust by group id\n",
    "    model_clu = model.get_robustcov_results(cov_type=\"cluster\", groups=g_id)\n",
    "    return {\n",
    "        \"beta_hat\": model.params[1],\n",
    "        \"se_conventional\": model.bse[1],\n",
    "        \"se_cluster\": model_clu.bse[1]\n",
    "    }\n",
    "\n",
    "def sim_cluster_many(R=1000, G=100, m=10, beta=1.0, tau=1.2, sigma=1.0):\n",
    "    out = [sim_cluster_once(G=G, m=m, beta=beta, tau=tau, sigma=sigma) for _ in range(R)]\n",
    "    bhats = np.array([d[\"beta_hat\"] for d in out])\n",
    "    se_conv = np.array([d[\"se_conventional\"] for d in out])\n",
    "    se_clu  = np.array([d[\"se_cluster\"] for d in out])\n",
    "    return {\n",
    "        \"emp_sd_beta_hat\": bhats.std(ddof=1),\n",
    "        \"mean_se_conventional\": se_conv.mean(),\n",
    "        \"mean_se_cluster\": se_clu.mean()\n",
    "    }\n",
    "\n",
    "res_cluster = sim_cluster_many(R=1000, G=100, m=10, beta=1.0, tau=1.2, sigma=1.0)\n",
    "res_cluster\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ac1835",
   "metadata": {},
   "source": [
    "## Correlated errors\n",
    "I simulated data where observations were grouped and shared the same random shock (so their errors were correlated). The regular OLS SEs again came out too low, while the cluster-robust SEs were much closer to the true variation. When I tried a simple bootstrap that ignored the group structure, it also underestimated the SE. So if errors are related, you have to use cluster-robust or block bootstrap methods to get it right."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6da5641",
   "metadata": {},
   "source": [
    "Show that if the correlation between coefficients is high enough, then the estimated standard deviation of the coefficient, using bootstrap errors, \n",
    "might not match that found by a full simulation of the Data Generating Process.  (This can be fixed if you have a huge amount of data for the bootstrap simulation.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf22c653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'empirical_SD_DGP': np.float64(0.16722904846233982),\n",
       " 'SE_naive_IID_boot': np.float64(0.15280290929995982),\n",
       " 'SE_cluster_boot': np.float64(0.18480915168299267)}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np, statsmodels.api as sm\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "def dgp_cluster(G=40, m=8, beta=1.0, tau=3.0, sigma=1.0):\n",
    "    \"\"\"Strong intra-cluster correlation: rho = tau^2 / (tau^2 + sigma^2) ‚âà 0.9.\"\"\"\n",
    "    n = G*m\n",
    "    g = np.repeat(np.arange(G), m)\n",
    "    X = rng.normal(0, 1, n)\n",
    "    u = rng.normal(0, tau, G)         # cluster shock\n",
    "    v = rng.normal(0, sigma, n)       # idiosyncratic\n",
    "    e = u[g] + v\n",
    "    Y = beta*X + e\n",
    "    return Y, X, g\n",
    "\n",
    "def iid_resid_boot_se(Y, X, B=800):\n",
    "    X1 = sm.add_constant(X)\n",
    "    base = sm.OLS(Y, X1).fit()\n",
    "    resid = base.resid\n",
    "    betas = []\n",
    "    for _ in range(B):\n",
    "        e_star = rng.choice(resid, size=len(resid), replace=True)  # WRONG under dependence\n",
    "        Y_star = base.fittedvalues + e_star\n",
    "        betas.append(sm.OLS(Y_star, X1).fit().params[1])\n",
    "    return np.std(betas, ddof=1)\n",
    "\n",
    "def cluster_boot_se(Y, X, groups, B=800):\n",
    "    # Resample entire clusters (block bootstrap)\n",
    "    X1 = sm.add_constant(X)\n",
    "    base = sm.OLS(Y, X1).fit()\n",
    "    betas = []\n",
    "    gids = np.unique(groups)\n",
    "    idx_by_g = {g: np.where(groups==g)[0] for g in gids}\n",
    "    for _ in range(B):\n",
    "        sample_g = rng.choice(gids, size=len(gids), replace=True)\n",
    "        idx = np.concatenate([idx_by_g[g] for g in sample_g])\n",
    "        betas.append(sm.OLS(Y[idx], X1[idx]).fit().params[1])\n",
    "    return np.std(betas, ddof=1)\n",
    "\n",
    "def empirical_sd_over_dgp(R=800, **dgp_kwargs):\n",
    "    bhats = []\n",
    "    for _ in range(R):\n",
    "        Y, X, g = dgp_cluster(**dgp_kwargs)\n",
    "        bhats.append(sm.OLS(Y, sm.add_constant(X)).fit().params[1])\n",
    "    return np.std(bhats, ddof=1)\n",
    "\n",
    "G, m = 40, 8\n",
    "Y, X, g = dgp_cluster(G=G, m=m)\n",
    "emp_sd = empirical_sd_over_dgp(R=800, G=G, m=m)\n",
    "se_iid  = iid_resid_boot_se(Y, X, B=800)\n",
    "se_clust= cluster_boot_se(Y, X, g, B=800)\n",
    "\n",
    "{\"empirical_SD_DGP\": emp_sd, \"SE_naive_IID_boot\": se_iid, \"SE_cluster_boot\": se_clust}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53f2993c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'empirical_SD_DGP_big': np.float64(0.03560541140860346),\n",
       " 'SE_naive_IID_boot_big': np.float64(0.039213296878995545),\n",
       " 'SE_cluster_boot_big': np.float64(0.03751764290581541)}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Larger sample: more clusters and/or size per cluster\n",
    "G, m = 400, 20\n",
    "Y_big, X_big, g_big = dgp_cluster(G=G, m=m)\n",
    "emp_sd_big = empirical_sd_over_dgp(R=400, G=G, m=m)\n",
    "se_iid_big  = iid_resid_boot_se(Y_big, X_big, B=400)\n",
    "se_clust_big= cluster_boot_se(Y_big, X_big, g_big, B=400)\n",
    "\n",
    "{\"empirical_SD_DGP_big\": emp_sd_big, \"SE_naive_IID_boot_big\": se_iid_big, \"SE_cluster_boot_big\": se_clust_big}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2682934b",
   "metadata": {},
   "source": [
    "When errors are highly correlated, a na√Øve IID residual bootstrap underestimates the standard error compared to a full DGP simulation. A cluster/block bootstrap (or cluster-robust SEs) matches the empirical SD much better. With huge datasets, even the na√Øve bootstrap‚Äôs bias shrinks, but the principled fix is to respect the dependence structure in the bootstrap."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
